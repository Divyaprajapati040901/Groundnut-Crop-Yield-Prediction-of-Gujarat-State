# -*- coding: utf-8 -*-
"""GROUNDNUT_YIELD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MjiNYudYiY89dp1dbHHkl5oVlElT7S3z
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor
from xgboost import XGBRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

df = pd.read_csv("/content/Final_Data_update.csv")
df.sample(5)

df.shape

df.columns

df = df.drop("Area_", axis=1)

df = df.drop("Production_", axis=1)

numerical = df.select_dtypes(include=["int", "float"])

df_numerical = pd.DataFrame(numerical)
df_numerical.head()

df_numerical.corr()

# Function for creating Heat map:
def create_heatmap(data):
    plt.figure(figsize=(10,6))
    sns.heatmap(data.corr(), annot=True, fmt=".2f", cmap='coolwarm')
    plt.title('Heatmap for Columns')
    plt.show()

create_heatmap(df_numerical)

# Extract the "Year" and "Production" columns
Year = df["Year"]
Area = df["Area"]

# Plotting
plt.figure(figsize=(8, 6))
plt.plot(Year, Area, marker='o', linestyle='-', color = 'darkblue')
plt.title('Area per Year')
plt.xlabel('Year')
plt.xticks(rotation=90)
plt.ylabel('Area')
plt.grid(True)
plt.xticks(Year)  # Set x-axis ticks to display all years
plt.tight_layout()

# Show plot
plt.show()

# Extract the "Year" and "Production" columns
Year = df["Year"]
Production = df["Production"]

# Plotting
plt.figure(figsize=(8, 6))
plt.plot(Year, Production, marker='o', linestyle='-', color = 'magenta')
plt.title('Production per Year')
plt.xlabel('Year')
plt.xticks(rotation=90)
plt.ylabel('Production (tonnes)')
plt.grid(True)
plt.xticks(Year)  # Set x-axis ticks to display all years
plt.tight_layout()

# Show plot
plt.show()

# Group by year and sum the production
production_per_year = df.groupby('Year')['Production'].sum()

# Plot
plt.figure(figsize=(10, 6))
production_per_year.plot(kind='bar', color='purple')
plt.title('Production per Year')
plt.xlabel('Year')
plt.ylabel('Production (tonnes)')
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Scatter plot
plt.scatter(df_numerical['Production'], df_numerical['Yield_(tonnes/ha)'], s=32, alpha=0.8, color = 'green')

# Adjusting spines
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)

# Axes labels
plt.xlabel('Production')
plt.ylabel('Yield (tonnes/ha)')

# Show the plot
plt.show()

categorical_features = ['Year', 'District']
numerical_features = ['Area', 'Production', 'NDVI', 'EVI', 'LAI','FAPAR','GPP','Rainfall','Temperature','SMI']

df_copy = df
df_copy.sample(5)

label_encoder = LabelEncoder()

df_copy[categorical_features] = df_copy[categorical_features].apply(lambda col: label_encoder.fit_transform(col))

df_copy.sample(5)

X = df_copy.drop(columns=['Yield_(tonnes/ha)'])
y = df_copy['Yield_(tonnes/ha)']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create different Regression models:
Linear_regression = LinearRegression()
Ridge_Regression =  Ridge()
Lasso_Regression =  Lasso(alpha = 0.1)
Decision_Tree =  DecisionTreeRegressor()
Random_Forest = RandomForestRegressor()
Gradient_Boosting = GradientBoostingRegressor()
AdaBoost = AdaBoostRegressor()
XGBoost = XGBRegressor()
svr = SVR()

model_list = [Linear_regression, Ridge_Regression, Lasso_Regression, Decision_Tree, Random_Forest, Gradient_Boosting, AdaBoost, XGBoost, svr]

model_name = ["Linear regression", "Ridge Regression", "Lasso Regression", "Decision Tree", "Random Forest", "Gradient Boosting", "AdaBoost", "XGBoost", "svr"]

# Creating list of accuracy of all the models and selecting best model with highest accuracy:
accuracy_list = []
for model in model_list:
  model.fit(X_train_scaled, y_train)
  y_pred = model.predict(X_test_scaled)
  accuracy = r2_score(y_test, y_pred)
  accuracy_list.append(accuracy)

# Creating dataframe of model with their respective accuracy:
model_df = pd.DataFrame({"Model": model_name, "Accuracy": accuracy_list})
model_df

# MODEL WITH ACCURACY

colors = ["INDIGO", "DARKORCHID", "mediumorchid", "violet", "orchid", "plum"]
model_df.plot.bar(x='Model', y='Accuracy', color = colors, edgecolor ='black')

# Add title
plt.title('Model Accuracy')

# Show plot
plt.show()

param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2]
}

grid_search_xgb = GridSearchCV(XGBoost, param_grid_xgb, cv=5, verbose=2)
grid_search_xgb.fit(X_train_scaled, y_train)

best_params_xgb = grid_search_xgb.best_params_
print("Best hyperparameters:", best_params_xgb)

XGBoost = XGBRegressor(learning_rate = 0.1, max_depth = 3, n_estimators = 300)
XGBoost.fit(X_train_scaled, y_train)
final_xgb_y = XGBoost.predict(X_test_scaled)

final_xgb_accuracy = r2_score(y_test, final_xgb_y)
final_xgb_accuracy

param_grid_gb = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.05, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search_gb = GridSearchCV(estimator=Gradient_Boosting, param_grid=param_grid_gb, cv=5)
grid_search_gb.fit(X_train_scaled, y_train)

best_params_gb = grid_search_gb.best_params_
print("Best hyperparameters:", best_params_gb)

Gradient_Boosting = GradientBoostingRegressor(learning_rate = 0.2, max_depth = 3, min_samples_leaf = 2,min_samples_split = 10, n_estimators = 200)
Gradient_Boosting.fit(X_train_scaled, y_train)
final_gb_y = Gradient_Boosting.predict(X_test_scaled)

final_gb_accuracy = r2_score(y_test, final_gb_y)
final_gb_accuracy

accuracy_scores = [
    0.749976,
    0.758049,
    0.727212,
    0.872531,
    0.832916,
    0.8962464652601693,
    0.774174,
    0.9157052302276633,
    0.823192
]

import numpy as np
import matplotlib.pyplot as plt

# Create scatter plot of actual and predicted values
plt.scatter(y_test, final_xgb_y, color="green", label="Actual vs. Predicted")

# Plotting the line of best fit
z = np.polyfit(y_test, final_xgb_y, 1)
p = np.poly1d(z)
plt.plot(y_test, p(y_test), color="red", label="Fitting Line")

# Give x and y labels and title to the plot
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Predictions vs. Actual Values".upper())

# Show legend
plt.legend()

# Show the plot
plt.show()

model_name = ["Linear regression", "Ridge Regression", "Lasso Regression", "Decision Tree", "Random Forest", "Gradient_Boosting", "AdaBoost", "XGBoost", "svr"]

# CREATING NEW DATAFRAME OF MODEL WITH THEIR RESPECTIVE ACCURACY AFTER HYPERPARAMETER TUNING

model_df1 = pd.DataFrame({"Model": model_name, "Accuracy": accuracy_scores})
model_df1

# MODEL WITH ACCURACY

colors = ['lightpink','pink','hotpink','deeppink','mediumvioletred',]
model_df1.plot.bar(x='Model', y='Accuracy', color = colors, edgecolor = 'black')

# Add title
plt.title('Model Accuracy')

# Show plot
plt.show()

import pickle

pickle.dump(XGBoost, open("Groundnut_xgb.pkl", 'wb'))